---
title: Quantile Regression in the Presence of Monotone Missingness with Sensitivity
  Analysis
author: "Minzhao Liu"
date: "March 24th, 2021"
job         : 'Supervisor: Dr. Mike Daniels. Department of Statistics, University of Florida'
license     : by-nc-sa
bibliography: qr-introduction.bib 
output: 
  # revealjs::revealjs_presentation:
  #   smaller: true
  #   highlight: pygments
  #   self_contained: false
  #   theme: black
  #   reveal_plugins: ["notes", "chalkboard"]
   
  ioslides_presentation:
    widescreen: true
    smaller: true

  # xaringan::moon_reader:
  #   lib_dir: libs
  #   nature:
  #     highlightStyle: github
  #     countIncrementalSlides: false

  # slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

```

## {data-background="assets/img/xmind.png"}

<div class="notes">

1. give introduction and review on , present existing methods, common issues
2. demonstrate our method, first start with ; then what we propose for the error distribution;
and how we embed the marginal QR in the settings;     and what's the computation difficulty; and how we deal with MAR and MNAR; then illustrate what simulation we did  ; and how it was applied on real data analysis
3. finally conclusion/summary will be provided. 

</div>

## Introduction: Why Quantile Regression

<style>
.container{
    display: flex;
}
.col{
    flex: 1;
}
</style>

<div class="container">
<div class="col">

  ![](assets/img/engel.png)

</div>

<div class="col">
  - Engel data on food expenditure vs household income for a sample of 235 19th century working class Belgian households.
  - $\tau$: 5%, 10%, 25%, 75%, 90%, 95%
  - Median regression
  - Mean regression
  - Increasing trend from mean regression
  - More info from QR
    - Slope change
    - Skewness
  - Less sensitive to heterogeneity and outliers
  - More complete description of the conditional distribution

</div>

</div>

<div class="notes">
</div>

## Introduction of Quantile Regression 

### Quantile (unconditional)

$$
Q_{Y}(\tau) = \inf \{y: F(y) \geq \tau \},
$$

### Quantile Regression (conditional with covariates)

$$
Q_{Y}(\tau|\mathbf x) = \mathbf x' \beta(\tau).
$$


<div class="notes">
- here's the definitions for tao's quantile 
- when we do regression, we want to find the best coefficient $\gamma$ so that the linear predictor is responses'  tao's quantile 
- since we can do various quantile regression on the same data, we may get more complete .. or get a whole picture than a single mean regression
</div>


## Traditional Frequentist Methods

- R package `quantreg` 
- Using simplex for linear programming problems mentioned in [@koenker1982] 
$$
\mathbf \beta(\tau) = \arg \min_b \sum_{i=1}^{n} \rho_{\tau}(y_{i} - \mathbf x_{i}' b)
$$
$$ \rho_{\tau} (u) = u(\tau - I(u < 0))
$$
* No distributional assumptions
* Fast using linear programming
* Asymptotic inference may not be accurate for small sample sizes
* Easy to generalize:
  * Random effects
  * $L_1$ , $L_2$ penalties

<div class="notes">
- Now we came to existing method for QR. the most common way for freq method is the quantreg package as in Koenker 1982
- it does not assume any distribution assumption and just use smplext to find the estimates minimizing the target function. 
- it's fast using linear programming
- but its asymptotic inferce 
- on the other hand, it's easy to generalize the application, for example, we may add random effects, or add L1, or L2 penalities like LASSO
</div>

## Bayesian Methods 

- [@yu2001]: asymmetric Laplace distribution (ALD) for QR under Bayesian framework
- [@hanson2002]: mixture of Polya tree prior for median regression on survival time in AFT model
- [@kottas2009]: semi-parametric QR models using mixtures of DP for the error distribution
- [@reich2010]: an infinite mixture of two Gaussian densities for error

## Common Issues

- Single quantile regression each time
- Densities have their restrictive mode at the quantile of interest, which is not appropriate when extreme quantiles are being investigated
- Quantile lines monotonicity constraints
- Joint inference is poor since it does not borrow information through single quantile regressions
- Not coherent to pool from every individual quantile regression, because the sampling distribution of $Y$ for $\tau_1$ is usually different from that under quantile $\tau_2$ since they are assuming different error distribution under two different quantile regressions [@tokdar2011]

<div class="notes">
- Asymmetric Laplace distribution = RQ
- here is the general introduction and summaries. next we will move to our main topic
</div>

## My Thesis

1. Bayesian Quantile Regression Using a Mixture of Polya Trees Priors
2. **Quantile Regression in the Presence of Monotone Missingness with Sensitivity Analysis using Finite Mixture of Normals**
3. Bayesian Approach with Dirichlet Process Mixture


## Monotone Missingness

A missing data pattern is monotone if, for each individual, there exists a measurement occasion $j$
such that and $R_1= ..= R_{j-1} = 1$ and $R_j = R_{j+1} = ... = R_{J} = 0$; that is, all responses are
observed through time $j-1$, and no responses are observed thereafter. $S$ is called *follow-up*
time.

SUBJECT | T1       | T2 | T3| T4| S
--------|----------|----|---|---|---
Subject 1| $Y_{11}$ | $Y_{12}$ |  | | 2
Subject 2| $Y_{21}$ | $Y_{22}$ |  $Y_{23}$ | $Y_{24}$ | 4

- Assumption: $Y_{i1}$ is always observed.
- Interested: $\tau$-th **marginal** quantile regression coefficients $\gamma_j = (\gamma_{j1}, \gamma_{j2}, ..., \gamma_{jp})^T$,
$$
\Pr(Y_{ij} \le \mathbf{x}^{T}_{i} \gamma_j) = \tau, \textrm{for } j = 1, ..., J,
$$
$$
p_k(Y) = p(Y|S = k), p_{\ge k}(Y) = p(Y | S \ge k)
$$

<div class="notes">
- concepts/Framework/settings/definition , familiar, but still refresh memories. 
- relative to intermittent missing data
- drop out time
- notations; subscription
</div>


## Pattern Mixture Model

- Mixture models factor the joint distribution of response and missingness as
$$
  p (\mathbf y, \mathbf S |\mathbf x, \mathbf \omega) = p (\mathbf y|\mathbf S, \mathbf x, \mathbf \omega) p (\mathbf S | \mathbf x, \mathbf \omega).
$$
- The full-data response distribution is given by
$$
  p (\mathbf y | \mathbf x, \mathbf \omega) = \sum_{S \in \mathcal{S}} p(\mathbf y| \mathbf S, \mathbf x, \mathbf \theta) p (\mathbf S | \mathbf x, \mathbf \phi),
$$
where $\mathcal{S}$ is the sample space for dropout time $S$ and the parameter vector $\mathbf \omega$ is partitioned as $(\mathbf \theta, \mathbf \phi)$.
- Furthermore, the conditional distribution of response within patterns can be decomposed as
$$
  P (Y_{obs}, Y_{mis} | \mathbf S, \mathbf \theta) = P
  (Y_{mis}|Y_{obs}, \mathbf S, \mathbf \theta_E) P(Y_{obs} | \mathbf S, \mathbf
  \theta_{y, O}),
$$

- $\mathbf \theta_E$:  extrapolation distribution
- $\mathbf \theta_{y, O}$ : distribution of observed responses

<div class="notes">
- next concept is  (PMM)
- relative to intermittent missing data
- drop out time
- notations; subscription
</div>

## Our Model Settings

- Sequential finite ($K$) mixture of normal distributions (MN) within each pattern
- Finite ($K$) mixture of normal distributions (MN) 
$$
p(x|\mathbf \theta) = \sum_{i = 1}^K \omega_i \phi_N(x; \mu_i, \sigma_i^2),
$$

- For simplicity, we may assume a single normal with each pattern just for illustration
$$
  \begin{array}{l}
      \displaystyle Y_{i1}|S_i=k \sim \mbox{MN}(\Delta_{i1} +  \mathbf{x_{i}^T\beta_1^{(k)}}, \sigma) , k = 1, \ldots, J,\\
       \displaystyle        Y_{ij}|\mathbf Y_{ij^{-}}, S_i=k \sim
      \begin{cases}
        \mbox{MN} (\Delta_{ij} + \mathbf y_{ij^{-}}^T \mathbf \beta_{y,j-1}, \sigma), & k \geq j ;  \\
        \mbox{MN} (\chi(\mathbf x_{i}, \mathbf y_{ij^{-}}), \sigma), & k < j ;  \\
      \end{cases}, \mbox{ for } 2 \leq j \leq J,  \\
       S_{i} = k \sim \textrm{Multinomial}(1, \mathbf \phi),
 \end{array}
$$

- $\chi(\mathbf x_i, \mathbf y_{ij^-})$ is the mean of the unobserved data distribution and allows sensitivity analysis by varying assumptions on $\chi$. For computational reasons, we assume that $\chi$ is linear in $y_{ij^{-}}$.

$$
\chi(\mathbf x_i, \mathbf y_{ij^-}) = \Delta_{ij} + \mathbf y_{ij^{-}}^T \mathbf \beta_{y,j-1} + h_0^{(k)}
$$

## Our Model Settings | Embed Marginal Quantile Regression Coefficients

- The marginal quantile regression models:
$$
  Pr (Y_{ij} \leq \mathbf x_{ij}^T \mathbf \gamma_j ) = \tau,
$$




## Identifiability/Over-parametrization?

- For (standard) identifiability of the distribution of the observed data, we use the
following restrictions (without loss of generality),
$$
 \sum_{k=1}^J \beta_{1l}^{(k)} = 0.
$$
- Also in order to not confound the marginal quantile regression parameters,
we put the following constraint on the parameters $\mathbf\theta$ in the mixture of normal distribution,
$$
\sum_{l= 1}^K \omega_{l}\mu_{l} = 0.
$$

## $\Delta$

$\Delta_{ij}$ are subject/time specific intercepts implicitly determined by the parameters in the model and are determined by marginal quantile regressions $\Delta_{ij} \sim (\tau, \mathbf x_{i},
\mathbf \beta, \mathbf h, \mathbf \theta, \mathbf \gamma_j, \mathbf \phi)$ , 

$$
\begin{equation}
  \tau = \mbox{Pr} (Y_{ij} \leq \mathbf x_{i}^T \mathbf \gamma_j ) = \sum_{k=1}^J
  \phi_k\mbox{Pr}_k (Y_{ij} \leq \mathbf x_{i}^T \mathbf \gamma_j ) \mbox{  for  } j = 1,
\end{equation}
$$

and
$$
\begin{align}
  \tau &= \mbox{Pr} (Y_{ij} \leq \mathbf x_{i}^{T} \mathbf \gamma_j ) =
  \sum_{k=1}^J
  \phi_k\mbox{Pr}_k (Y_{ij} \leq \mathbf x_{i}^{T} \mathbf \gamma_j ) \\
  & = \sum_{k=1}^J \phi_k \int\cdots \int \mbox{Pr}_k (Y_{ij} \leq \mathbf
  x_{i}^{T} \mathbf \gamma_j | \mathbf y_{ij^{-}}
  ) \mbox{Pr}_k (y_{i(j-1)}| \mathbf y_{i(j-1)^{-}})  \nonumber \\
  & \quad \cdots \mbox{pr}_k (y_{i2}| y_{i1}) \mbox{pr}_k(y_{i1})
  dy_{i(j-1)}\cdots dy_{i1}.  \mbox{  for  } j = 2, \ldots, J .\nonumber
\end{align}
$$



## Idea Recap

<!-- setup -->
<!-- recall -->
<!-- tricky part  -->

- Pattern mixture model to jointly model the response and missingness (@dh2008, @little1994)
- A sequential finite mixture of normals within each pattern
- Explicitly model the quantile regression coefficients of interest (through constraints)
- Maximum likelihood method to get the estimates and bootstrap (@efron1993) method to construct confidence interval and make inferences. 
- Use the BIC criterion to select the number of components in
the mixture of normals

## Computation | Calculation of $\Delta$ ($j=1$)

$\Delta_{i1}$:
if we expand the constraint, 
$$
\tau = p(Y_{ij} \leq \mathbf x_{i}^T \mathbf \gamma_j ) = \sum_{k=1}^J
  \phi_kp_k (Y_{ij} \leq \mathbf x_{i}^T \mathbf \gamma_j ) \mbox{  for  } j = 1,
$$
$$
    \tau = \sum_{k = 1}^J \phi_k \left( \sum_{l = 1}^{K} \omega_{l} \Phi \left( \frac{\mathbf x_{i}^T
        \mathbf \gamma_1 - \Delta_{i1} -\mathbf{x_i^T \beta_1^{(k)}} - \mu_{l}}{ \sigma_{l} } \right) \right).
$$  
  where $\Phi$ is the standard normal CDF.
  Because the RHS of the above equation  is continuous and monotone in $\Delta_{i1}$, it can be solved by a
  standard numerical root-finding method (e.g. bisection method) with
  minimal difficulty.

## Computation |  Calculation of $\Delta$ ($j>1$) 

$$
\begin{align}
  \tau &= p(Y_{ij} \leq \mathbf x_{i}^{T} \mathbf \gamma_j ) =
  \sum_{k=1}^J
  \phi_k p_k (Y_{ij} \leq \mathbf x_{i}^{T} \mathbf \gamma_j ) \\
  & = \sum_{k=1}^J \phi_k \int\cdots \int p_k (Y_{ij} \leq \mathbf
  x_{i}^{T} \mathbf \gamma_j | \mathbf y_{ij^{-}}
  ) p_k (y_{i(j-1)}| \mathbf y_{i(j-1)^{-}})  \nonumber \\
  & \quad \cdots pr_k (y_{i2}| y_{i1}) pr_k(y_{i1})
  dy_{i(j-1)}\cdots dy_{i1}.  \mbox{  for  } j = 2, \ldots, J .\nonumber
\end{align}
$$

## Computation |  Calculation of $\Delta$ ($j>1$) | Lemma 1

An integral of a normal CDF with mean $b$ and standard deviation
$a$ over another normal distribution with mean $\mu$ and standard
deviation $\sigma$ can be simplified to a closed form in terms of
normal CDF:
$$
      \int \Phi \left( \frac{x-b}{a} \right) d\Phi(x; \mu, \sigma)  =
      \begin{cases}
        1- \Phi \left( \frac{b-\mu}{\sigma} \big /
          \sqrt{\frac{a^2}{\sigma^2}+1} \right) & a > 0, \\
        \Phi \left( \frac{b-\mu}{\sigma} \big /
          \sqrt{\frac{a^2}{\sigma^2}+1} \right) & a < 0,
      \end{cases}
$$
where $\Phi(x; \mu, \sigma)$ stands for a CDF of normal
    distribution with mean $\mu$ and standard deviation $\sigma$.
  
- by recursively applying Lemma 1 $(j-1)$ times,
  each multiple integral can be
  simplified to single normal CDF. Thus we can easily solve for
  $\Delta_{ij}$ using standard numerical root-finding method as for $j= 1$.

## Computation | MLE

- The observed data likelihood for an individual $i$ with follow-up time
$S_i = k$ is
$$
\begin{align} L_i(\mathbf \xi| \mathbf y_i, S_{i} = k) & =
  \phi_k p_k (y_{ik} | y_{i1}, \ldots, y_{i(k-1)})
  p_k (y_{i(k-1)}|y_{i1}, \ldots, y_{i(k-2)}) \cdots p_{k} (y_{i1}) \\
  & = \phi_k p_{\geq k} (y_{ik} | y_{i1}, \ldots, y_{i(k-1)}) p_{\geq k-1}
  (y_{i(k-1)}|y_{i1}, \ldots, y_{i(k-2)}) \cdots p_{k} (y_{i1}), \nonumber
\end{align}
$$
where $\mathbf y_i = (y_{i1}, \ldots, y_{ik})$.

- To facilitate computation of the $\Delta$'s and the likelihood, we propose the tricky way in the previous slides to obtain analytic forms for the required integrals.
- However, it could still be computational intensive, thus we call Fortran in R function to compute the main part to speed up
- Use the bootstrap to construct confidence interval and make inferences.


## Idea Summary

- Embed the marginal quantile regressions directly in the model through constraints in the
likelihood of pattern mixture models
$$
  p (\mathbf y | \mathbf x, \mathbf \omega) = \sum_{S \in \mathcal{S}} p(\mathbf y| \mathbf S, \mathbf x, \mathbf \theta) p (\mathbf S | \mathbf x, \mathbf \phi),
$$

- The finite mixture of normals distribution makes model flexible and accommodates
heavy tails, skewness, and multi-modality. 
- The mixture model also allows sensitivity analysis for the missing data, say MNAR.

## Missing Data Mechanism and Sensitivity Analysis

- Mixture models are not identified due to insufficient information provided by observed data.
- Specific forms of missingness are needed to induce constraints to identify the distributions
for incomplete patterns, in particular, the extrapolation distribution
- In mixture models with monotone missingness, MAR holds (@molen1998, @wang2011) if and only if, for each $j \ge 2$ and $k < j$: 
$$
p_k(y_j|y_1, \ldots, y_{j-1}) = p_{\geq j}(y_j|y_1, \ldots, y_{j-1}).
$$

<div class="notes">
$$
S = 1 , P_{S=1}(Y_2 | Y_1) \mbox{ is unknown and } = P_{S=2} ( Y_2 | Y_1)
$$
</div>


## Sensitivity Analysis

$$
  \begin{array}{l}
           \displaystyle        Y_{ij}|\mathbf Y_{ij^{-}}, S_i=k \sim
      \begin{cases}
        \mbox{N} (\Delta_{ij} + \mathbf y_{ij^{-}}^T \mathbf \beta_{y,j-1}, \sigma), & k \geq j ;  \\
        \mbox{N} (\Delta_{ij} + \mathbf y_{ij^{-}}^T \mathbf \beta_{y,j-1} + h_0^{(k)}, \sigma), & k < j ;  \\
      \end{cases}, \mbox{ for } 2 \leq j \leq J,  
 \end{array}
$$

- When $2\le j \le J$ and $k <j$, $Y_j$ is not observed, thus $h_0^{(k)}$ can not be identified from the observed data
- $\mathbf \xi_s = (\mathbf h^{(k)}: k=1,\ldots,J-1)$ is a set
of sensitivity parameters (@dh2008).
- $\mathbf \xi_s = \mathbf \xi_{s0} = \mathbf 0$, MAR holds.
- $\mathbf \xi_s$ is fixed at $\mathbf \xi_s \neq \mathbf \xi_{s0}$, MNAR. 
- We can vary $\mathbf \xi_s$ around $\mathbf 0$ to examine the impact of different MNAR assumptions

<div class="notes">
- Y components are beyond the observed ones; i.e not observed, 

</div>

## Simulation | Comparator 

- *rq* function in (@quantreg) [Denote as RQ]
  - no distributional assumption
  - does not accommodate MAR or MNAR missingness
- Bottai's algorithm (@bottai2013) [Denote as BZ]
  - no distributional assumption
  - assumes MAR missingness, but does not allow MNAR missingness
  - impute missing outcomes using the estimated conditional quantiles of missing outcomes given observed data
  
## Simulation | Settings

- Bivariate responses: $Y_{i1}$ were always observed, while some of $Y_{i2}$ were missing.
- Three Scenarios: 
  1. $Y_2$ were missing at random and we used the MAR assumption in our algorithm.
  2. $Y_2$ were missing not at random, we misspecified the MDM for our algorithm and still assumed MAR,
  3. $Y_2$ were missing not at random, we used the correct MNAR MDM,
- For each scenario, we considered four error distributions: normal, student t distribution with 3 degrees of freedom, Laplace distribution. 
- For each dataset, we fit quantile regression for quantiles $\tau =$ 0.1, 0.3, 0.5, 0.7, 0.9.
- Comparison Criteria: Mean squared error
$$
  \mbox{MSE} (\gamma_{ij}) = \frac{1}{100} \sum_{k = 1}^{100}
  \left( \hat{\gamma}_{ij}^{(k)}  - \gamma_{ij}\right)^2, i = 0, 1,
$$

## Simulation | Performance

- Under all errors and all scenarios (including the wrong MDM in scenario 2), our model
has the lowest MSE for almost all regression coefficients

- large gains over *rq*, especially for each marginal quantile of $Y_2$ under missingness
- BZ does much better than rq function for missing data because it imputes missing responses under MAR
- Advantages over BZ for inference on most quantile regressions especially when the tails of
the distribution are non-standard (e.g., heavy tails and skewness) since we assume a
multivariate mixture of normals distribution for each component in the pattern mixture
model

## Real Data Analysis: Tours 

SUBJECT | 6 MONTHS       | 18 MONTHS | AGE | RACE | BASELINE
--------|----------|----|---|---|---
Subject 1| $Y_{11}$ | $Y_{12}$ | $x_{11}$  | $x_{12}$| $Y_{10}$
Subject 2| $Y_{21}$ | $Y_{22}$ |  $x_{21}$ | $x_{22}$ | $Y_{20}$

- Weights were recorded at baseline ($Y_{0}$ ), 6 months ($Y_1$ ) and 18 months ($Y_2$ ).
- We are interested in how the distributions of weights at six months and eighteen months
change with covariates.
- The regressors of interest include **AGE**, **RACE** (black and white) and **weight at baseline** ($Y_0$).
Weights at the six months ( $Y_1$) were always observed and 13 out of 224 observations (6%)
were missing at 18 months ( $Y_2$).
- The **AGE** covariate was scaled to 0 to 5 with every increment representing 5 years.
- We fitted regression models for bivariate responses for quantiles (10%, 30%,
50%, 70%, 90%).
- We ran 1000 bootstrap samples to obtain 95% confidence intervals.

## Results


<div class="container">
<div class="col">

  ![](assets/img/tours.png)

</div>

<div class="col">
- For weights of participants at six months,
weights of whites are generally 4kg lower
than those of blacks for all quantiles
significantly.
- Weights of participants are not affected by
age significantly.
- Coefficients of baseline weight show a
strong relationship with weights after 6
months.
- For weights at 18 months after baseline,
we have similar results.
- However, whites do not weigh significantly
less than blacks at 18 months unlike at 6
months.

</div>

</div>

## Sensitivity Analysis

We also did a sensitivity analysis based on an assumption of MNAR.

- Based on previous studies of pattern of weight regain after lifestyle treatment (@wadden2001, @perri2008extended) we assume that

$$
  E(Y_2 - Y_1| R=0) = 3.6 \mbox{kg},
$$

which corresponds to 0.3kg regain per month after finishing the initial 6-month program.

- Specify $\chi(\mathbf x_i, Y_{i1})$as 
$$
\chi(\mathbf x_i, y_{i1}) = 3.6 + y_{i1}
$$

- There are no large differences for estimates for under MNAR vs MAR.
- This is partly due to the low proportion of missing data in this study.

## Conclusion

- A marginal quantile regression model for data with monotone missingness
- Pattern mixture model to jointly model the full data
response and missingness
- Allows for sensitivity analysis which is essential for the analysis of
missing data
- Extended to Bayesian framework with Dirichlet Process mixture model

## References 

<style>
slides > slide { overflow: scroll; }

</style>

<div id="refs"></div>


